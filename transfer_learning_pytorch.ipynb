{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30c30145",
   "metadata": {},
   "source": [
    "# Transfer Learning in PyTorch\n",
    "Written by Calden Wloka for CS 153\n",
    "\n",
    "This notebook draws heavily on the [official PyTorch transfer learning tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) by Sasank Chilamkurthy.\n",
    "\n",
    "Some other extremely useful documentation you may find useful:\n",
    "- [Saving and loading models](https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html)\n",
    "    - You often need an object to persist across training environments or instances. This allows you to work around XSEDE's timeout limitations, or run multiple experiments at different times with the same model.\n",
    "- [Torchvision object detection finetuning tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)\n",
    "    - This is a more advanced tutorial looking at fine tuning a model to a new dataset. In particular, it shows you how to set up a custom data loader.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9945fa",
   "metadata": {},
   "source": [
    "# Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7afc3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from PIL import Image\n",
    "import itertools\n",
    "\n",
    "\n",
    "from helpers import show_img\n",
    "\n",
    "from prep_data import process_img\n",
    "\n",
    "from preprocess import generate_normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc39564",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19384b34",
   "metadata": {},
   "source": [
    "# Custom Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3026ad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/lbp'\n",
    "# data_dir = 'data/augmented'\n",
    "\n",
    "# load_path = 'Models/pollen-model.pt'\n",
    "load_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3af572",
   "metadata": {},
   "source": [
    "# Load Data/Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817e758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf17ceff",
   "metadata": {},
   "source": [
    "This tutorial trains a classifier to differentiate between *ants* and *bees*. The dataset is available [here](https://download.pytorch.org/tutorial/hymenoptera_data.zip).\n",
    "\n",
    "If you download and extract the dataset, you will notice that it is rather small; there are about 120 training images each for ants and bees (along with 75 validation images). It would be hard to train a classifier from scratch on this amount of data, but transfer learning can leverage pre-training from prior data.\n",
    "\n",
    "Furthermore, we will make use of some basic data augmentation for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17826680",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('Models/'): os.mkdir('Models/')\n",
    "\n",
    "if load_path is not None:\n",
    "  checkpoint = torch.load(load_path, map_location=device)\n",
    "\n",
    "  data_transforms = checkpoint['data_transforms']\n",
    "  dataloaders = checkpoint['dataloaders']\n",
    "  class_names = dataloaders['train'].dataset.classes\n",
    "\n",
    "  dataset_sizes = {x: len(dataloaders[x].dataset.imgs) for x in ['train', 'val']}\n",
    "else:\n",
    "  data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "      transforms.RandomResizedCrop(224),\n",
    "      transforms.RandomHorizontalFlip(),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "      transforms.Resize(256),\n",
    "      transforms.CenterCrop(224),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "  }\n",
    "\n",
    "\n",
    "  dataset = datasets.ImageFolder(data_dir)\n",
    "\n",
    "  train_size = int(0.8 * len(dataset))\n",
    "  val_size = len(dataset) - train_size\n",
    "\n",
    "  image_datasets = {}\n",
    "\n",
    "  train_data, val_data = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "  image_datasets['train'], image_datasets['val'] = train_data.dataset, val_data.dataset\n",
    "\n",
    "  image_datasets['train'].transform = data_transforms['train']\n",
    "  image_datasets['val'].transform = data_transforms['val']\n",
    "\n",
    "\n",
    "  dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                        shuffle=True, num_workers=4)\n",
    "        for x in ['train', 'val']}\n",
    "  dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "\n",
    "  class_names = image_datasets['train'].classes\n",
    "\n",
    "  print('The amount of data is:')\n",
    "  print(dataset_sizes)\n",
    "  print(' ')\n",
    "  print('The dataset dictionary is:')\n",
    "  print(image_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282c290b",
   "metadata": {},
   "source": [
    "Let's explore our data setup a little bit; make sure you understand what each part is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493e06a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classes are:')\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8971d18",
   "metadata": {},
   "source": [
    "Another imporant part of data handling is to *look at your data*! Ideally, we want to look at our output *after* our dataloader has processed it, as that can help catch inappropriate augmentations or other transformation or data handling errors.\n",
    "\n",
    "To look at data after the dataloader has processed it, we need to grab the tensors from the device and put them back in our regular workspace, and reshape them into the standard shape that we expect images to take. Our `tensor_show` function assumes the images have been pulled into our workspace, but takes care of the reshaping. We will use a `torchvision.utils` function called `make_grid` to turn a batch of images into one long image, and later we will use this function to visualize our predictions and explicitly send our images to the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e73fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor2numpy_img(inp):\n",
    "  inp = inp.numpy().transpose((1, 2, 0)) #usually start with batch size (different order)\n",
    "  # already transformed them, so need to undo to get them ready to displace\n",
    "  mean = np.array([0.485, 0.456, 0.406])\n",
    "  std = np.array([0.229, 0.224, 0.225])\n",
    "  inp = std * inp + mean\n",
    "  # clip them back\n",
    "  inp = np.clip(inp, 0, 1)\n",
    "  return inp\n",
    "\n",
    "\n",
    "def tensor_show(inp, title=None):\n",
    "  \"\"\"Imshow for Tensor.\"\"\"\n",
    "  inp = tensor2numpy_img(inp)\n",
    "  plt.imshow(inp)\n",
    "  if title is not None:\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b803ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "tensor_show(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebf11f2",
   "metadata": {},
   "source": [
    "Now that we have data handling set up, we want to set up our training routines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea753f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25, save_path='Models/pollen-model.pt'):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_epoch = 0\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # each epoch is a full run-through of training data\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode (compute gradient over any of the weights you told the network it can update)\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluation mode (so doesn't update weights, just gives prediction)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                # optimizer tells you how you are updating the weights\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                # scheduler updates your learning rate\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            # when you are in validation phase, look at accuracy over particular epoch, and compare with best accuracy thus far\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_epoch = epoch\n",
    "                best_acc = epoch_acc\n",
    "                # save weights (model.state_dict()) as best model weights\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    # XSEDE has timeout of 12 hrs or something\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f} (epoch #{best_epoch})')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    if save_path is not None:\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'data_transforms': data_transforms,\n",
    "            'dataloaders': dataloaders,\n",
    "        }, save_path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd0057b",
   "metadata": {},
   "source": [
    "Just like looking at our data can be useful, it is also a very good idea to inspect your model predictions and not just rely on the validation accuracy. For that, we want a visualization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643c03bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "  was_training = model.training\n",
    "  model.eval()\n",
    "  images_so_far = 0\n",
    "  fig = plt.figure()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "      inputs = inputs.to(device)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      outputs = model(inputs)\n",
    "      _, preds = torch.max(outputs, 1)\n",
    "\n",
    "      for j in range(inputs.size()[0]):\n",
    "        images_so_far += 1\n",
    "        ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'{class_names[labels[j]]}\\npredicted: {class_names[preds[j]]}')\n",
    "        tensor_show(inputs.cpu().data[j])\n",
    "\n",
    "        if images_so_far == num_images:\n",
    "          model.train(mode=was_training)\n",
    "          return\n",
    "    model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc3c119",
   "metadata": {},
   "source": [
    "Now that we have training routines defined, we can perform transfer learning! Note that the [original tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) demonstrates two different transfer learning protocols: finetuning the whole network (i.e. allowing weights throughout the network to change), and treating the network as a fixed feature extractor (i.e. only training the final classifier layer; sometimes this may be extended to multiple fully connected \"readout\" layers).\n",
    "\n",
    "For this demo we will focus on the latter style, but both options can be useful.\n",
    "We do this by setting the `requires_grad` parameter of the model feature layers to `False`, thereby preventing the gradient from being computed over them and leaving them open to updates by the training routine. Since newly constructed modules have `requires_grad=True` by default, when we declare our new output layer, it will be the only layer with a gradient calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11838de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    # tells the model not to change this part of network (don't compute gradient over weights)\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "# number of features activates in network, number classes\n",
    "model_conv.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "# load model onto device\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD is the most classic optimizer to use\n",
    "# tell optimizer learner rate (high big steps are to take)\n",
    "# momentum (default 0) additional hyperparameter that tries to make sure not too spikey (avg with weight to figure out how to update model when looking at errors)\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "# step_size\n",
    "# gamma is how much you decay by\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97757fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_path is not None:\n",
    "  model_conv.load_state_dict(checkpoint['model_state_dict'])\n",
    "  model_conv.eval()\n",
    "  model_conv.cuda()\n",
    "else:\n",
    "  model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d90b13",
   "metadata": {},
   "source": [
    "# Tools for Visualizing Results/Interpretation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1a9cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(filter(lambda method: not '__' in method, dir(model_conv.state_dict()))))\n",
    "\n",
    "visualize_model(model_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc16290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img_data(img_path):\n",
    "  path, filename = os.path.split(img_path)\n",
    "  label = os.path.split(path)[-1]\n",
    "\n",
    "  image = Image.open(img_path)\n",
    "\n",
    "  return label, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e6db50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(phase='val', with_input=True, with_targs=True, with_preds=True, with_loss=True, with_data_idx=False):\n",
    "  \"\"\" returns inputs, targs, preds, losses \"\"\"\n",
    "  res = []\n",
    "  input_imgs, targets, predictions, losses, data_idxs = [], [], [], [], []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for i, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "      inputs = inputs.to(device)\n",
    "      if with_input: input_imgs.extend([tensor2numpy_img(im) for im in inputs.cpu()])\n",
    "\n",
    "      labels = labels.to(device)\n",
    "      if with_targs: targets.append(labels.cpu().numpy())\n",
    "\n",
    "      batch_size = len(labels)\n",
    "\n",
    "      if with_data_idx: data_idxs.extend([i+idx for idx in range(batch_size)])\n",
    "\n",
    "      outputs = model_conv(inputs)\n",
    "      if with_loss:\n",
    "        loss = [round(criterion(torch.unsqueeze(outputs[idx], dim=0), torch.unsqueeze(labels[idx], dim=0)).item(), 2) for idx in range(len(labels))]\n",
    "        losses.extend(loss)\n",
    "\n",
    "      _, preds = torch.max(outputs, 1)\n",
    "      if with_preds: predictions.append(preds.cpu().numpy())\n",
    "\n",
    "  if with_input: res.append(np.array(input_imgs))\n",
    "  if with_targs: res.append(np.concatenate(targets))\n",
    "  if with_preds: res.append(np.concatenate(predictions))\n",
    "  if with_loss: res.append(np.array(losses))\n",
    "  if with_data_idx: res.append(np.array(data_idxs))\n",
    "\n",
    "  if len(res) > 1: return tuple(res)\n",
    "  else: return res[0]\n",
    "\n",
    "\n",
    "def predict(image, with_class=False, with_output=False):\n",
    "  res = []\n",
    "\n",
    "  transformed = data_transforms['val'](image).float()\n",
    "  transformed = transformed.unsqueeze_(0)\n",
    "\n",
    "  input_img = transformed.to(device)\n",
    "  output = model_conv(input_img)\n",
    "\n",
    "  idx = output.data.cpu().numpy().argmax()\n",
    "  res.append(idx)\n",
    "  if with_class: res.append(class_names[idx])\n",
    "  if with_output: res.append(output)\n",
    "  \n",
    "  if len(res) > 1: return tuple(res)\n",
    "  else: return res[0]\n",
    "\n",
    "\n",
    "def get_loss(image, label):\n",
    "  pred, output = predict(image, with_output=True)\n",
    "\n",
    "  loss = round(criterion(output, torch.tensor([label], device=device)).item(), 2)\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbb7bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired by fastai (https://docs.fast.ai/interpret.html)\n",
    "class Interpretation():\n",
    "  def __init__(self, model):\n",
    "    print('initialized')\n",
    "\n",
    "    self.model = model\n",
    "    self.class_names = class_names\n",
    "    self.class_ct = len(self.class_names)\n",
    "\n",
    "    self.cm = None\n",
    "\n",
    "  \n",
    "  def get_class_idx(self, class_name):\n",
    "    return self.class_names.index(class_name)\n",
    "\n",
    "\n",
    "  def get_class_data(self, class_idx, phase='val', with_data_idx=False):\n",
    "    if with_data_idx: y_data, y_targs, y_preds, y_losses, y_idxs = get_preds(phase=phase, with_input=True, with_targs=True, with_preds=True, with_loss=True, with_data_idx=True)\n",
    "    else: y_data, y_targs, y_preds, y_losses = get_preds(phase=phase, with_input=True, with_targs=True, with_preds=True, with_loss=True)\n",
    "\n",
    "    class_targs_idxs = np.argwhere(y_targs == class_idx)\n",
    "\n",
    "    res = [y_data[class_targs_idxs].squeeze(), y_targs[class_targs_idxs].squeeze(), y_preds[class_targs_idxs].squeeze(), y_losses[class_targs_idxs].squeeze()]\n",
    "    if with_data_idx: res.append(y_idxs[class_targs_idxs].squeeze())\n",
    "\n",
    "    return tuple(res)\n",
    "  \n",
    "\n",
    "  def get_top_losses(self, ct=None, phase='val', class_idx=None, with_data_idx=False, loss_thresh=None):\n",
    "    if class_idx is None:\n",
    "      info = get_preds(phase=phase, with_input=True, with_targs=True, with_preds=True, with_loss=True, with_data_idx=with_data_idx)\n",
    "\n",
    "      if with_data_idx: y_data, y_targs, y_preds, y_losses, y_idxs = info\n",
    "      else: y_data, y_targs, y_preds, y_losses = info\n",
    "    else:\n",
    "      info = self.get_class_data(class_idx, phase=phase, with_data_idx=with_data_idx)\n",
    "\n",
    "      if with_data_idx: y_data, y_targs, y_preds, y_losses, y_idxs = info\n",
    "      else: y_data, y_targs, y_preds, y_losses = info\n",
    "    \n",
    "    dists = abs(y_preds - y_targs)\n",
    "\n",
    "    errors = (y_preds - y_targs != 0)\n",
    "\n",
    "    err_ct = len(np.argwhere(errors==True))\n",
    "\n",
    "    error_idxs = np.argwhere(errors==True)\n",
    "\n",
    "    sorted_dists = np.argsort(dists, axis=0)\n",
    "\n",
    "    print('number of errors:', len(np.argwhere(errors==True)), 'correct:', len(np.argwhere(errors==False)))\n",
    "\n",
    "    sorted_losses = np.argsort(y_losses, axis=0)\n",
    "\n",
    "    if ct is None or err_ct < ct: ct = err_ct\n",
    "\n",
    "    if loss_thresh is None: top_errors = sorted_losses[-ct:]\n",
    "    else: top_errors = np.argwhere(y_losses > loss_thresh)\n",
    "\n",
    "    res = [y_data[top_errors].squeeze(), y_targs[top_errors].squeeze(), y_preds[top_errors].squeeze(), y_losses[top_errors].squeeze()]\n",
    "    if with_data_idx: res.append(y_idxs[top_errors].squeeze())\n",
    "\n",
    "    return tuple(res)\n",
    "\n",
    "\n",
    "  def plot_top_losses(self, ct=None, phase='val', class_idx=None, loss_thresh=None):\n",
    "    y_data, y_targs, y_preds, y_losses = self.get_top_losses(ct, phase=phase, class_idx=class_idx, loss_thresh=loss_thresh)\n",
    "\n",
    "    img_targs = [self.class_names[targ] for targ in y_targs]\n",
    "    img_preds = [self.class_names[pred] for pred in y_preds]\n",
    "\n",
    "    img_titles = [f'Label: {targ}\\nPrediction: {pred}\\n(loss: {loss})' for targ, pred, loss in zip(img_targs, img_preds, y_losses)]\n",
    "\n",
    "    show_img(list(y_data), title=img_titles, suptitle=(None if class_idx is None else class_names[class_idx]), targ=img_targs, pred=img_preds)\n",
    "\n",
    "\n",
    "  def confusion_matrix(self, save=False):\n",
    "    cm = torch.zeros(self.class_ct, self.class_ct)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for i, (inputs, classes) in enumerate(dataloaders['val']):\n",
    "        inputs = inputs.to(device)\n",
    "        classes = classes.to(device)\n",
    "        outputs = model_conv(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        for t, p in zip(classes.view(-1), preds.view(-1)):\n",
    "          cm[t.long(), p.long()] += 1\n",
    "\n",
    "    #option to store confusion matrix with so don't have to re-do it every time we plot\n",
    "    if save: self.cm = cm\n",
    "\n",
    "    return cm\n",
    "\n",
    "\n",
    "  def plot_confusion_matrix(self, plot_txt=True, cmap='Blues'):\n",
    "    if self.cm is None: cm = self.confusion_matrix()\n",
    "    else: cm = self.cm\n",
    "\n",
    "    fig = plt.figure(figsize=(16, 10), dpi=100) # (12, 8)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title('Confusion Matrix')\n",
    "    tick_marks = np.arange(self.class_ct)\n",
    "    plt.xticks(tick_marks, self.class_names, rotation=90)\n",
    "    plt.yticks(tick_marks, self.class_names, rotation=0)\n",
    "\n",
    "    if plot_txt:\n",
    "      thresh = cm.max() / 2.\n",
    "      for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        coeff = f'{int(cm[i, j])}'\n",
    "        plt.text(j, i, coeff, horizontalalignment=\"center\", verticalalignment=\"center\", color=\"white\"\n",
    "                  if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "        ax = fig.gca()\n",
    "        ax.set_ylim(self.class_ct-.5,-.5)\n",
    "\n",
    "        # plt.tight_layout()\n",
    "        plt.ylabel('Actual')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.grid(False)\n",
    "\n",
    "\n",
    "  def per_class_accuracy(self, sort=True):\n",
    "    cm = self.confusion_matrix()\n",
    "\n",
    "    class_accuracy = cm.diag()/cm.sum(1)\n",
    "\n",
    "    if sort:\n",
    "      class_accuracy = class_accuracy.cpu().numpy()\n",
    "      acc_idxs = np.argsort(class_accuracy)[::-1]\n",
    "\n",
    "      acc_sorted = class_accuracy[acc_idxs]\n",
    "      classes_sorted = np.array(self.class_names)[acc_idxs]\n",
    "\n",
    "      for class_name, acc in zip(classes_sorted, acc_sorted):\n",
    "        print(f'class: {class_name} / accuracy: {round(acc.item(), 4)}')\n",
    "\n",
    "      return acc_sorted\n",
    "    else:\n",
    "      for i, acc in enumerate(class_accuracy):\n",
    "        print(f'class: {self.class_names[i]} / accuracy: {round(acc.item(), 4)}')\n",
    "\n",
    "      return class_accuracy\n",
    "\n",
    "  \n",
    "  def least_accurate(self, ct=1):\n",
    "    class_accuracy = self.per_class_accuracy(sort=False).cpu().numpy()\n",
    "\n",
    "    acc_idxs = np.argsort(class_accuracy)\n",
    "\n",
    "    acc_sorted = class_accuracy[acc_idxs]\n",
    "    classes_sorted = np.array(self.class_names)[acc_idxs]\n",
    "\n",
    "    for class_name, acc in zip(classes_sorted, acc_sorted):\n",
    "      print(f'class: {class_name} / accuracy: {round(acc.item(), 4)}')\n",
    "\n",
    "    return classes_sorted[:ct], acc_sorted[:ct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7a5dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "interp = Interpretation(model_conv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2200804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the confusion matrix\n",
    "\n",
    "cm = interp.confusion_matrix(save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18abf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "\n",
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b13428d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# per-class accuracy, sorted from highest to lowest score\n",
    "\n",
    "per_class_acc = interp.per_class_accuracy(sort=True)\n",
    "\n",
    "new_score = per_class_acc[:-2].sum() / (len(class_names)-2)\n",
    "\n",
    "\n",
    "new_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fae1214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize predictions with highest losses\n",
    "\n",
    "interp.plot_top_losses(ct=10)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "838d95e32c0a9945b042675061ca13a341e347dc99b88dfd07b3f46486702d29"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('pollen_project': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
